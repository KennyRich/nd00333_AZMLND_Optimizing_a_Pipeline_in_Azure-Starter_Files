# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
### Problem Statement
The project main goal is to accurately predict(classify) the clients that will subscribe to a term deposit and those that would not. The dataset used for this project is the `Bank Marketing Dataset` from the UCI machine learning repository. 

### Solution
The solution is to build a machine learning model using two difference approaches. One of the appraoch is using the `Microsoft azure hyperdrive` which gave an accuracy of 91.67%. The second approach was using the `Microsoft AutoML` which gave an accuracy of 91.74%. The best performing model was a Voting ensemble moddel which was produced by the AutoML approach.
![Hyperdrive Accuracy](hyperdrive-accuracy.png)
![AutoML Accuracy](automl-accuracy.png)

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The data is loaded, preprocessed (which includes one hot encoding, binary encoding of categorical features, parsing of the dates) and splitted in the [train.py file](train.py). The [training script](train.py) is the entry point for the SKLearn esitmator which is fed into the configuration of the hyperdrive. Logistic regression is the algorithm used in training the model. It is a predictive algorithm using independent variables to predict the dependent variable, just like Linear Regression, but with a difference that the dependent variable should be categorical variable. There are various hyperparameters in logistic regression such as `C` (which is the inverse of the regularization strength, smaller valeus depicts stronger regularization), `max_iter` (which is the number of iterations before the solver converges). The default penalty for logistic regression is the `l2 regularization` and the default solver being `lbfgs`. The Hyperdrive then runs with the aim of maximising the accuracy of the model after being passed with various algorithm parameters (mainly `C` and `max_iter`) to vary from.  

**What are the benefits of the parameter sampler you chose?**
`RandomSampler` is used because it is not computationally expensive, hence we can hasten the exploration of the parameter space. 
This is because it randomly draws parameter from a set of defined search space unlike other forms like grid search or bayesian sampling.

**What are the benefits of the early stopping policy you chose?**
`BanditPolicy` helps to know the minimun required improved needed to continue with the parameter search. It uses the slack factor which defines how far off the primary metric of a run must be from the best performing run's for it to be terminated.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The AutoML is set to have 5 cross validations and it runs 100 iterations and takes in maximum of eight concurrent runs at a time. the target column and the data is also passed into the configuration of the AutoML. The autoML model ran various models and the model generated with the best accuracy is the `VotingEnsemble` model with an accuracy of 91.74%. Though while training the AutoML model raised a concern about data imbalance. The hyperparameters generated by the model are: n_estimators, min_samples_leaf, min_weight_fraction_leaf, colsample_bylevel, max_depth and so many other hyperparameters.These hyperparameters are XGboost algorithm parameters which are finely tuned by the AutoML model to give accurate results. 

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The AutoML model had an accuracy of about 0.9174 compared to a model accuracy of 0.9167. the difference in both models accuracy are quite insignificant. 
The architecture of both models were kind of different. For the hyperdrive model the data was cleaned and then pass into the a logistic regression model and then into the hyperdrive for model tuning whereas in the AutoML model, the cleaned data is just passed into the AutoML configuration and hence the AutoML could explore other algorithms to train the dataset and even perform some other approaches like stacking and ensembling the algorithms.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
In the future, one of the things to be included into the whole pipeline would be handling imbalanced data as this could be one of the cause of the model's very high accuracy. So in the future we could look at a weighted approach ton handle the case if imbalanced dataset and we use other form of metrics such as `precision`,`recall` or `AUROC` to determine the model performance.

Also, the data cleaning and feature engineering or interaction pipeline should be separate from the training pipeline as feature engineering and interaction takes a lot of experimentation. A good feature engineering could help to boost the model's accuracy greatly and hence improve the classification accuracy of the model.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
![cluster-deletion](cluster-deletion.png)
